{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPdHQNmsPssEanj8bj+NnxD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Regression\n"],"metadata":{"id":"JxqYSCTive-g"}},{"cell_type":"markdown","source":["1.What is Simple Linear Regression?\n","  - Simple Linear Regression is a statistical method used to model the relationship between two variables:\n","\n","     - Independent variable (X): This is the predictor variable.\n","     - Dependent variable (Y): This is the variable we want to predict.\n","\n"],"metadata":{"id":"n3ywuwfzv37e"}},{"cell_type":"markdown","source":["2.What are the key assumptions of Simple Linear Regression?\n","  - Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear. This means that the change in Y is proportional to the change in X.\n","  - Independence: The observations are independent of each other. This means that the value of one observation does not affect the value of another observation.\n","  - Homoscedasticity: The variance of the errors (residuals) is constant across all levels of the independent variable. This means that the spread of the data points around the regression line is consistent.\n","  - Normality: The errors are normally distributed. This means that the distribution of the errors follows a bell-shaped curve.\n"],"metadata":{"id":"lKN1TKk6v3ov"}},{"cell_type":"markdown","source":["3.What does the cofficient m represent in the equation Y=mX+c?\n","  - In the context of Simple Linear Regression:\n","\n","    - Slope (m) represents the change in the dependent variable (Y) for a one-unit change in the independent variable (X).\n","    - It indicates the direction and steepness of the relationship between X and Y."],"metadata":{"id":"Y3T6CmWkv3e3"}},{"cell_type":"markdown","source":["4.What does the intercept c represent in the equation Y=mX+c?\n","  - In the context of Simple Linear Regression:\n","\n","    - Intercept (c) represents the predicted value of the dependent variable (Y) when the independent variable (X) is 0.\n","    - It is the point where the regression line crosses the y-axis."],"metadata":{"id":"M_sSwp7Kv3Vv"}},{"cell_type":"markdown","source":[" 5.What do we calculate the slope m in Simple Linear Regression?\n","   - m = (Σ(Xi - X̄)(Yi - Ȳ)) / Σ(Xi - X̄)²\n","   - Where:\n","\n","     - Xi: Individual values of the independent variable\n","     - X̄: Mean of the independent variable\n","     - Yi: Individual values of the dependent variable\n","     - Ȳ: Mean of the dependent variable\n","    - Σ: Summation symbol"],"metadata":{"id":"h8wZohlhv3MX"}},{"cell_type":"markdown","source":["6.What is the purpose of the leaast squares method in Simple Linear Regression?\n","  - The least squares method is a technique used to estimate the values of 'm' and 'c' that minimize the sum of the squared differences between the observed values of Y and the predicted values of Y based on the regression line.\n","\n"],"metadata":{"id":"CRbQ_GXOv3Cf"}},{"cell_type":"markdown","source":["7.How is the coefficient of determination (R2) interpreted in Simple Linear Regression?\n","  - The coefficient of determination, denoted as R², is a statistical measure that represents the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X) in a regression model.\n","\n","  - Interpretation:\n","\n","   - R² values range from 0 to 1.\n","  - An R² of 0 indicates that the model does not explain any of the variability in the dependent variable.\n","  - An R² of 1 indicates that the model explains all of the variability in the dependent variable.\n","  - In general, a higher R² value indicates a better fit of the model to the data."],"metadata":{"id":"-UilYb_vv25I"}},{"cell_type":"markdown","source":["8.What is Multiple Linear Regression?\n","  - Multiple Linear Regression is a statistical technique that uses two or more independent variables to predict the outcome of a single dependent variable. It's an extension of Simple Linear Regression, which uses only one independent variable."],"metadata":{"id":"JmzCyEjRv2uc"}},{"cell_type":"markdown","source":["9.What is the main difference between Simple and Multiple Linear Regression?\n","  - The primary distinction lies in the number of independent variables used to predict the dependent variable.\n","\n","   - Simple Linear Regression: Uses only one independent variable to predict the dependent variable. It explores the relationship between two variables, assuming a linear association.\n","\n","   - Multiple Linear Regression: Uses two or more independent variables to predict the dependent variable. It explores the relationship between multiple variables and the dependent variable, assuming a linear relationship between each independent variable and the dependent variable, while holding other independent variables constant."],"metadata":{"id":"86HJZlwFv2kE"}},{"cell_type":"markdown","source":["10.What are the key assumptions of Multiple Linear Regression?\n","  - Linearity: There should be a linear relationship between the dependent variable and each independent variable. This means that the change in the dependent variable is proportional to the change in the independent variable.\n","\n","  - No Multicollinearity: The independent variables should not be highly correlated with each other. Multicollinearity can make it difficult to interpret the individual effects of each independent variable on the dependent variable.\n","\n","  - Independence of Errors: The errors (residuals) should be independent of each other. This means that the value of one error should not be related to the value of another error.\n","\n","  - Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. This means that the spread of the data points around the regression line should be consistent.\n","\n","  - Normality of Errors: The errors should be normally distributed. This means that the distribution of the errors should follow a bell-shaped curve."],"metadata":{"id":"lT3-AVWwv2Zp"}},{"cell_type":"markdown","source":["11.What is heteroscedasticity, and how does if affect the result of a Multiple Linear Regression model?\n","  - Heteroscedasticity refers to a situation where the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables. In simpler terms, it means that the spread or variability of the data points around the regression line is not consistent.\n","  - While heteroscedasticity does not introduce bias in the coefficient estimates, it can have several negative consequences for the Multiple Linear Regression model:\n","\n","     - Inefficient Estimates\n","\n","    - Invalid Hypothesis Tests\n","\n","    - Misleading p-values\n","\n","    - Reduced Predictive Accuracy\n","\n"],"metadata":{"id":"tvmVTwwvv2Py"}},{"cell_type":"markdown","source":["12.How can you improve a Multiple Linear Regression model with high multicollinearity?\n","  - Remove one or more of the highly correlated variables\n","  -Combine highly correlated variables\n","\n","  - Use Principal Component Analysis (PCA)\n","\n","  - Regularization techniques\n","\n","  - Collect more data\n","\n","  - Centering the variables\n","\n"],"metadata":{"id":"EI1O_NrYv2FW"}},{"cell_type":"markdown","source":["13.What are some common techniques for transforming categorical variable for use in regression models?\n","  - One-Hot Encoding: Create binary (0/1) dummy variables for each category.\n","  - Label Encoding: Assign a unique numerical label to each category.\n","  - Ordinal Encoding: Assign numerical values based on the inherent order of categories.\n","  - Binary Encoding: Represent categories using binary code.\n","  - Target Encoding: Replace categories with the mean of the target variable for that category."],"metadata":{"id":"kxebibcjv18P"}},{"cell_type":"markdown","source":["14.What is the role of interaction terms in Multiple Lineare Regression?\n","  - Interaction terms capture how the effect of one variable on the outcome depends on the level of another variable. They allow for modeling non-additive relationships, improving predictive accuracy and providing deeper insights."],"metadata":{"id":"t5D_hu6qv1yk"}},{"cell_type":"markdown","source":["15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n","  - In Simple Linear Regression, the intercept is the predicted value of Y when X is 0. In Multiple Linear Regression, the intercept is the predicted value of Y when all independent variables are 0."],"metadata":{"id":"LBWjJ-8Sv1mp"}},{"cell_type":"markdown","source":["16.What is the significance of the slope in regression analysis, and how does it affect predictions?\n","  - The slope represents the change in the dependent variable for a one-unit change in the independent variable. It directly affects predictions by determining the steepness of the relationship and the magnitude of change in the outcome."],"metadata":{"id":"cY_o7N12v1WS"}},{"cell_type":"markdown","source":["17.How does the intercept in a regression model provide context for the relationship between variable?\n","  - The intercept provides the baseline value of the dependent variable when all independent variables are zero. This baseline helps in understanding the scale and shift of the relationship between variables, giving context to their impact."],"metadata":{"id":"sXMOWg0gv1Os"}},{"cell_type":"markdown","source":["18.What are the limitations of using R2 as a sole measure of model performance?\n","  - R² doesn't indicate if a model is biased or if predictions are accurate. It can increase with irrelevant predictors and doesn't reflect model complexity or overfitting."],"metadata":{"id":"6JaowueIv08S"}},{"cell_type":"markdown","source":["19.How would you interpret a large standard error for a regression coefficient?\n","  - A large standard error for a regression coefficient suggests that the estimate of the coefficient is imprecise and there is substantial uncertainty about its true value. This could indicate that the variable is not a strong predictor, there's high variability in the data, or multicollinearity is present."],"metadata":{"id":"b31rA4Nsv0ww"}},{"cell_type":"markdown","source":["20.How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n","  - Identification: Heteroscedasticity is often identified in residual plots by a funnel or cone shape, where the spread of residuals increases or decreases as the predicted values change.\n","\n","  - Importance: Heteroscedasticity can lead to inefficient and unreliable estimates of regression coefficients, invalidating statistical tests and potentially leading to incorrect conclusions. Addressing it improves the validity and reliability of the regression model."],"metadata":{"id":"yi0BF7TXv0ji"}},{"cell_type":"markdown","source":["21.What does it mean if a Multiple Linear Regression model has a high R2 but low adjusted R2?\n","  - It suggests that the model might have too many irrelevant predictors. While a high R² indicates the model explains a large portion of the variance in the dependent variable, the low adjusted R² indicates this high R² might be artificially inflated due to the inclusion of these unnecessary variables."],"metadata":{"id":"wZ0cXtXEv0Ym"}},{"cell_type":"markdown","source":["22.Why is it important to scale variable in Multiple Linear Regression?\n","  - Scaling ensures that all variables have a similar range and magnitude, preventing variables with larger values from dominating the model and improving the interpretability and stability of the regression coefficients, especially when using regularization techniques or algorithms sensitive to feature scaling."],"metadata":{"id":"WIEBfKnov0NW"}},{"cell_type":"markdown","source":["23.What is polynomial regression?\n","  - Polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as an nth degree polynomial in x. It's a special case of linear regression where the relationship between the independent and dependent variables is modeled by a polynomial function."],"metadata":{"id":"syGdwoX4v0Al"}},{"cell_type":"markdown","source":["24.How does polynomial regression differ from linear regression?\n","  - Linear regression models a straight-line relationship between variables, while polynomial regression models a curved relationship by introducing polynomial terms (like x², x³) to the equation. This allows for capturing non-linear patterns in the data that linear regression can't."],"metadata":{"id":"yrIdRIMxvz06"}},{"cell_type":"markdown","source":["25.When is polynomial regression used?\n","  - Polynomial regression is used when the relationship between the independent and dependent variables is non-linear. It is useful for fitting a curve to data that doesn't follow a straight line, allowing for more accurate predictions and a better representation of the underlying relationship."],"metadata":{"id":"Qsyg70lNvzp_"}},{"cell_type":"markdown","source":["26.What is the general equation for polynomial regression?\n","  - y = b0 + b1x + b2x^2 + ... + bnx^n\n","  - Where:\n","\n","     - y is the dependent variable\n","     - x is the independent variable\n","     - b0, b1, b2, ..., bn are the coefficients\n","     - n is the degree of the polynomial"],"metadata":{"id":"l3pZsvLovzfI"}},{"cell_type":"markdown","source":["27.Can polynomial regression be applied to multiple variable?\n","  - Yes, polynomial regression can be applied to multiple variables by including polynomial terms for each variable and interaction terms between variables in the regression equation. This allows for modeling non-linear relationships between the dependent variable and multiple independent variables."],"metadata":{"id":"ppCUD1zWvzTv"}},{"cell_type":"markdown","source":["28.What are the limitations of pliynomial regression?\n","  - Overfitting: Polynomial regression with a high degree can overfit the training data, leading to poor generalization to new data.\n","  - Outliers: Outliers can have a significant impact on the fitted curve.\n","  - Interpretation: Higher-degree polynomial models can be difficult to interpret.\n","  - Computational Cost: The computational cost of fitting a polynomial model increases with the degree of the polynomial."],"metadata":{"id":"F7bOG_IYvzID"}},{"cell_type":"markdown","source":["29.What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n","  - R-squared (R²) and Adjusted R-squared: Higher values generally indicate a better fit, but overfitting can inflate R². Adjusted R² accounts for model complexity.\n","  - Root Mean Squared Error (RMSE): Measures the average difference between predicted and actual values; lower is better.\n","  - Cross-validation: Evaluate the model on different subsets of the data to assess its generalization ability.\n","  - Visual inspection of the fitted curve: Ensure the curve reasonably follows the data trend without e"],"metadata":{"id":"NspuWjOAvy51"}},{"cell_type":"markdown","source":["30.Why is visualization important in polynomial regression?\n","  - Visualization helps in assessing the fit of the polynomial curve to the data, identifying potential overfitting or underfitting, and understanding the relationship between the variables, aiding in selecting the appropriate degree of the polynomial for the model."],"metadata":{"id":"iZPv02r2vyY1"}},{"cell_type":"markdown","source":["31.How is polynomial regression implemented in Python?\n","  - You can implement polynomial regression using libraries like scikit-learn (PolynomialFeatures for transforming features and LinearRegression for fitting the model) or NumPy for manual calculations. These tools provide functionalities for creating polynomial features, fitting the regression model, and making predictions."],"metadata":{"id":"6amHrpbd2Nx0"}}]}